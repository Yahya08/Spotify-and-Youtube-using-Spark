# -*- coding: utf-8 -*-
"""UTS Bigdata_21.11.4365_Yahya Handarestanto.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VLPjgGCidnBuGFLXJpda4IJ2W6R5-fpY

*   Nama  : Yahya Handarestanto
*   Nim   :  21.11.4365
*   kelas : 21.IF08

# 1) Dengan menggunakan Python dan Spark, unduh dataset berikut ini
https://www.kaggle.com/datasets/salvatorerastelli/spotify-and-youtube/data

a. Baca dan pahami dokumentasi dari dataset yang tersedia di website
tersebut.

b. Panggil dan tampilkan lima baris pertama dari dataset tersebut, lalu
buat program untuk memeriksa tipe data dari masing-masing kolom,
menghitung jumlah null, mendapatkan nilai deskripsi secara umum,
dan melakukan perbaikan pada penamaan kolom.
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install pyspark

from pyspark.sql import SparkSession
from pyspark.sql.functions import max,min,col,year,to_date,when,count
from pyspark.sql import functions as F
spark = SparkSession.builder.getOrCreate()
sc = spark.sparkContext

spark = SparkSession.builder.appName("NamaAplikasi").getOrCreate()

# Gantilah ini dengan kode Anda untuk membaca DataFrame 'data' dari sumber data.
data = spark.read.csv("/content/drive/MyDrive/BIGDATA LANJUT/UTS/Spotify_Youtube.csv", header=True)

# Lakukan filter pada kolom '_c0' dengan menggunakan rlike
data_filter = data.filter(
    (col("Artist").isNotNull()) &  # Kolom Artist tidak boleh NULL
    (col("Url_spotify").isNotNull()) &  # Kolom Url_spotify tidak boleh NULL
    (col("Track").isNotNull()) &  # Kolom Track tidak boleh NULL
    (col("Album").isNotNull()) &  # Kolom Album tidak boleh NULL
    (col("Album_type").isNotNull()) &  # Kolom Album_type tidak boleh NULL
    (col("Uri").isNotNull()) &  # Kolom Uri tidak boleh NULL
    (col("Danceability") >= 0.7) &  # Contoh: Danceability harus >= 0.7
    (col("Energy") >= 0.5)  # Contoh: Energy harus >= 0.5
)

# Tampilkan hasil filter
data_filter.show()

# Jangan lupa untuk menutup sesi Spark jika Anda sudah selesai
spark.stop()

# Mengimpor modul yang diperlukan dari PySpark
from pyspark.sql.functions import expr

# Membaca data dari file CSV dengan menggunakan Spark
data = spark.read.csv('/content/drive/MyDrive/BIGDATA LANJUT/UTS/Spotify_Youtube.csv', header=True, inferSchema=True)

# Menampilkan tipe data dari 'data' yang telah dibaca
print("The type of data is", type(data))

# Menggunakan ekspresi (expr) untuk menyaring baris yang memiliki nilai '_c0' yang hanya terdiri dari digit
data_filter = data.filter(expr("_c0 rlike '^[0-9]*$'"))

# Menampilkan tipe data dari kolom-kolom dalam DataFrame 'data_filter'
data_filter.dtypes

"""**Deskripsi umum**"""

data_filter.summary()

"""**Perbaikan penamaan kolom**"""

# Mengimpor modul yang diperlukan dari PySpark
from pyspark.sql.functions import count, when, col

# Mengganti nama kolom "_c0" menjadi "Id" dalam DataFrame 'data_filter'
data_renamed = data_filter.withColumnRenamed("_c0", "Id")

# Menghitung jumlah nilai null dalam setiap kolom DataFrame yang telah diubah nama
data_null = data_renamed.select([count(when(col(c).isNull(), c)).alias(c) for c in data_renamed.columns])

# Menampilkan 5 baris pertama hasil perhitungan nilai null
data_null.show(5)

"""**Memeriksa data null**"""

data_renamed.show()

"""## 2) Lakukan beberapa eksplorasi data dengan"""

# Program 2: Menggunakan DataFrame asal dan memilih kolom yang tidak ingin ditampilkan
# Daftar kolom yang tidak ingin ditampilkan
excluded_columns = ['Id', 'Release_Date', 'Genre']

# Memilih kolom yang tidak termasuk dalam 'excluded_columns' dari DataFrame 'data_renamed'
data_sub = data_renamed.drop(*excluded_columns)

# Menampilkan hasil pemilihan kolom
data_sub.show()

# Menghitung jumlah nilai null dalam setiap kolom
data_null2 = data_sub.select([count(when(col(c).isNull(), c)).alias(c) for c in data_sub.columns])

# Menampilkan hasil perhitungan nilai null
data_null2.show()

"""**Menghapus data null**"""

data_clean = data_sub.dropna()

data_null2 =  data_clean.select([count(when(col(c).isNull(),c)).alias(c) for c in data_clean.columns])
data_null2.show()

"""Menghilangkan nilai view yang berbentuk string"""

from pyspark.sql.functions import col

# Filter data hanya untuk baris yang memiliki nilai Likes yang dapat diubah menjadi tipe data numerik
data_clean = data_clean.filter(col("Likes").cast("int").isNotNull())

# Menampilkan DataFrame yang sudah difilter
data_clean.show()

from pyspark.sql.functions import col

# Filter data hanya untuk baris yang memiliki nilai "Stream" yang hanya terdiri dari angka
data_clean2 = data_clean.filter(col("Stream").rlike('^[0-9]+$'))

# Menampilkan DataFrame yang sudah difilter
data_clean2.show()

data_clean2.dtypes

from pyspark.sql.types import (
    StringType, BooleanType, IntegerType, FloatType, DateType
)

from pyspark.sql.functions import col
from pyspark.sql.types import FloatType

# Daftar kolom yang ingin diubah tipe datanya
columns_to_cast = ["Acousticness", "Danceability", "Energy", "Loudness", "Speechiness", "Instrumentalness", "Liveness", "Valence", "Tempo", "Duration_ms", "Views", "Likes", "Comments"]

# Loop untuk mengubah tipe data kolom-kolom yang diinginkan
for col_name in columns_to_cast:
    data_clean_fix = data_clean2.withColumn(col_name, col(col_name).cast(FloatType()))

data_clean.printSchema()

data_clean_fix.show()

"""**a. Tampilkan ringkasan statistik dari atribut 'Danceability', 'Energy', dan
'Valence'. Berikan interpretasi atas distribusi nilai-nilai tersebut.**
"""

# Memilih kolom-kolom yang ingin dihitung statistiknya
selected_columns = ['Danceability', 'Energy', 'Valence']

# Menggunakan metode describe() untuk menghitung statistik
statistics_summary = data_clean_fix.select(selected_columns).describe()

# Menampilkan ringkasan statistik
statistics_summary.show()

""" **b. Identifikasi dan berikan daftar 5 lagu dengan 'Speechiness' tertinggi,
lalu berikan analisis singkat tentang karakteristik musik dari lagu-lagu
ini.**
"""

top_Speechiness=data_clean_fix.sort(col("Speechiness").desc())
top_Speechiness.show(5)

""" **c. Hitung rata-rata dari atribut 'Tempo' lagu-lagu dalam dataset. Berikan
insight tentang preferensi tempo dalam dataset ini.**
"""

from pyspark.sql.functions import mean

# Menghitung rata-rata dari kolom 'Tempo' dalam DataFrame
average_tempo = data_clean_fix.select(mean('Tempo')).collect()[0][0]

# Menampilkan hasil rata-rata
print("Rata-rata Tempo dalam dataset adalah:", average_tempo)

"""insight untuk lagu yang memiliki tempo rata" pasti memiliki Danceability yang tinggi Speechiness rendah dan memiliki durasi 3-4 menitan dan untuk lagu itu pasti memiliki view yang banyak"""

data_clean_fix.filter(col("Tempo") == 120).show()

"""**Bandingkan rata-rata durasi lagu untuk lagu-lagu yang memiliki tempo
di atas 120 BPM dengan yang di bawah atau sama dengan 120 BPM.**
"""

from pyspark.sql.functions import avg

# Filter lagu-lagu dengan tempo di atas 120 BPM
lagu_di_atas_120 = data_clean_fix.filter(data_clean_fix['Tempo'] > 120)

# Filter lagu-lagu dengan tempo di bawah atau sama dengan 120 BPM
lagu_di_bawah_atau_sama_dengan_120 = data_clean_fix.filter(data_clean_fix['Tempo'] <= 120)

# Hitung rata-rata durasi untuk lagu-lagu di atas 120 BPM
rata_rata_durasi_di_atas_120 = lagu_di_atas_120.select(avg('Duration_ms')).collect()[0][0]

# Hitung rata-rata durasi untuk lagu-lagu di bawah atau sama dengan 120 BPM
rata_rata_durasi_di_bawah_atau_sama_dengan_120 = lagu_di_bawah_atau_sama_dengan_120.select(avg('Duration_ms')).collect()[0][0]

# Menampilkan hasil perbandingan
print("Rata-rata durasi lagu di atas 120 BPM adalah:", rata_rata_durasi_di_atas_120)
print("Rata-rata durasi lagu di bawah atau sama dengan 120 BPM adalah:", rata_rata_durasi_di_bawah_atau_sama_dengan_120)

"""analisis
untuk tempo yang lebih dari rata" atau kurang dari rata" tidak akan mempengaruhi durasi dari lagu tersebut seperti data diatas hanya memiliki perbedaan 6000 ms

**e. Temukan lagu-lagu dengan tingkat speechiness di atas 0.5 dan analisis
apakah terdapat korelasi antara speechiness dan jumlah komentar di
YouTube.**
"""

korelasi = data_clean_fix.select('Title','Speechiness','Comments')
korelasi_filter=korelasi.filter(col('Speechiness')> 0.5)
korelasi_filter.show()

"""**Menghitung korelasi**"""

from pyspark.sql.functions import col
from pyspark.ml.stat import Correlation
from pyspark.ml.feature import VectorAssembler

# Mengubah tipe data kolom "Speechiness" menjadi float
korelasi_filter = korelasi_filter.withColumn("Speechiness", col("Speechiness").cast("float"))

# Menggabungkan kolom-kolom "Comments" dan "Speechiness" menjadi vektor fitur
vector_assembler = VectorAssembler(inputCols=["Comments", "Speechiness"], outputCol="features")
data_vector = vector_assembler.transform(korelasi_filter).select("features")

# Menghitung korelasi antara "Comments" dan "Speechiness"
correlation_matrix = Correlation.corr(data_vector, "features").head()[0]

# Menampilkan korelasi antara kedua kolom
print("Correlation between Comments and Speechiness:")
print(correlation_matrix)

"""analisis
antara Comments  dan Speechiness memiliki nilai korelasi yaitu -0.39296444 yang berarti antara coments dan Speechiness tidak memiliki hubungan.

## 3. Terapkan setidaknya 3 operasi RDD pada data tersebut seperti reduceByKey,
sortByKey, flatMap, dll. lalu berikan analisisnya.
"""

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = stopwords.words('english')

file_path = '/content/drive/MyDrive/BIGDATA LANJUT/UTS/data_True_clean/data_clean_fix.csv'

# Print the file_path
print("The file_path is", file_path)

# Create a fileRDD from file_path
fileRDD = sc.textFile(file_path)

# Check the type of fileRDD
print("The file type of fileRDD is", type(fileRDD))

# Simpan DataFrame ke dalam file CSV
data_clean_fix.write.csv('/content/drive/MyDrive/BIGDATA LANJUT/UTS/data_True_clean/data_clean_fix.csv', header=True, mode="overwrite")

"""**flatMap**"""

splitRDD = fileRDD.flatMap(lambda x: x.split())

"""**map**"""

# Ubah ke lowercase dan abaikan stop_words
splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)

# Buat tuple dari setiap kata
splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))

"""**reducebykey**"""

resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)

for word in resultRDD.take(10):
	print(word)

"""**sortByKey**"""

resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))

resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)

for word in resultRDD_swap_sort.take(10):
	print(word)

"""## Analisis

splitRDD: Program ini mengambil RDD (Resilient Distributed Dataset) fileRDD, memecah setiap baris teks menjadi kata-kata individu dengan membagi berdasarkan spasi, dan menghasilkan RDD baru yang berisi kata-kata.

splitRDD_no_stop: Setelah membagi teks menjadi kata-kata, program ini mengubah setiap kata menjadi huruf kecil dan memeriksa apakah kata tersebut termasuk dalam daftar kata berhenti (stop words). Kata-kata berhenti adalah kata-kata yang umumnya diabaikan dalam analisis teks karena mereka tidak memberikan wawasan yang signifikan. Kata-kata berhenti diabaikan dalam RDD ini.

splitRDD_no_stop_words: Program ini membuat pasangan kata dengan nilai 1, yang nantinya akan digunakan untuk menghitung frekuensi kemunculan kata-kata.

resultRDD: Program ini menggunakan reduceByKey untuk menggabungkan kata-kata yang sama dan menjumlahkan frekuensinya. Hasilnya adalah pasangan kata dan jumlah kemunculan kata tersebut dalam teks.

Loop for word in resultRDD.take(10) digunakan untuk mencetak 10 kata pertama yang muncul dalam RDD hasil.

resultRDD_swap: Program ini menukar kunci (jumlah kemunculan kata) dan nilai (kata itu sendiri) dalam pasangan kata. Ini dilakukan untuk memungkinkan pengurutan berdasarkan jumlah kemunculan kata.

resultRDD_swap_sort: RDD hasil dari langkah sebelumnya diurutkan berdasarkan jumlah kemunculan kata secara menurun (descending), sehingga kata yang paling sering muncul muncul di atas.

Loop for word in resultRDD_swap_sort.take(10) digunakan untuk mencetak 10 kata dengan frekuensi kemunculan tertinggi dalam teks.
"""